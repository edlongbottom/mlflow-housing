{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592a9d59",
   "metadata": {},
   "source": [
    "# Boston Housing: model serving with MLFlow\n",
    "\n",
    "This notebook is intended to demonstrate the use of MLFlow for deploying models as a prediction service. \n",
    "\n",
    "It follows on from the notebook in the Experimentation folder (BostonHousing_mlflow.ipynb) - picking up from where that notebook finished. Firstly, we will be using the MLFlow UI to select the best performing model and then deploying it.\n",
    "\n",
    "Multiple deployment methods are outlined:\n",
    "1. Deploying the model on a local REST server\n",
    "2. Deploying the model as a containerised service locally on Docker Desktop\n",
    "3. Deploying the model to a remote Kubernetes instance using Seldon Core\n",
    "\n",
    "In each case, the model is deployed as a RESTful web service. Curl has been used in each case to test the exposed API. \n",
    "\n",
    "**Goal:** *deploy a model as a web service for predicting house prices.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b5087",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a447611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from seldon_core.seldon_client import SeldonClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55baff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d9c52",
   "metadata": {},
   "source": [
    "### Review models in the MLFlow UI \n",
    "\n",
    "Start up a local tracking server and point it towards the experiment SQLite db (for entities) and local file storage (for artefacts) using the mlflow CLI. Then, navigate to http://localhost:5000/ in a browser to see the MLFlow UI and compare models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0629c2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'mlflow' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!mlflow server `\n",
    "    --backend-store-uri 'sqlite:///../experimentation/mlruns.db' `\n",
    "    --default-artifact-root ../experimentation/mlruns `\n",
    "    --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c704cf5f",
   "metadata": {},
   "source": [
    "Look through the runs in MLFlow and select the best performing model from the relevant experiment. Assign the associated experiment and run ids to their respective variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b91e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = '2'\n",
    "run_id = '2ce79d559d0546dd91bd26a252b0ac78'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b09e50",
   "metadata": {},
   "source": [
    "### Sense check the model\n",
    "\n",
    "Step to sense check the pipeline. \n",
    "Load the chosen pipeline manually and pass it a record from the raw test set to see if it generates a sensible prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adadc420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some data for testing (this is one record from the test set used in Experimentation)\n",
    "X_test_0 = np.array([5.86, 6.108, 330.0, 19.1, 9.16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e50c1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddlo\\Python\\Projects\\MLFlow-housing\\venv-housing\\lib\\site-packages\\sklearn\\base.py:445: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([21.13982272])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the pipeline and run it on the data\n",
    "poly_pipeline = joblib.load(f'../experimentation/mlruns/{experiment_id}/{run_id}/artifacts/model/model.pkl')\n",
    "poly_pipeline.predict(X_test_0.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32c34a",
   "metadata": {},
   "source": [
    "### Serve the model on a local REST server\n",
    "\n",
    "The model can be deployed with a local REST server to create a prediction web service. Use the mlflow CLI to serve your chosen model (include path to model) and expose it at port 1234.\n",
    "\n",
    "Note: this step requires Conda to be installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf6242",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow models serve -m ../experimentation/mlruns/1/6dca30cc19b44d359dbaf994cee1084a/artifacts/model -p 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce925d",
   "metadata": {},
   "source": [
    "Test the prediction web service using Curl or the python requests module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a9fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Curl to test the web service (shell)\n",
    "!curl -X POST -H \"Content-Type:application/json; format=pandas-split\" \\\n",
    "    --data '{\"columns\":[\"INDUS\",\"RM\",\"TAX\",\"PTRATIO\",\"LSTAT\"],\"data\":[[5.86, 6.108, 330.0, 19.1, 9.16]]}' http://127.0.0.1:1234/invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a55c1c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.139822721718414]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   112  100    20  100    92     20     92  0:00:01 --:--:--  0:00:01  3612\n"
     ]
    }
   ],
   "source": [
    "# use Curl to test the web service (windows dos)\n",
    "!curl -X POST -H \"Content-Type:application/json; format=pandas-split\" --data \"{\\\"columns\\\":[\\\"INDUS\\\",\\\"RM\\\",\\\"TAX\\\",\\\"PTRATIO\\\",\\\"LSTAT\\\"],\\\"data\\\":[[5.86, 6.108, 330.0, 19.1, 9.16]]}\" http://127.0.0.1:1234/invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36423f5c",
   "metadata": {},
   "source": [
    "### Deploy the model as a containerised service using Docker\n",
    "\n",
    "Build a docker image of the containerised model using the mlflow CLI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1344fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the docker image (shell)\n",
    "!mlflow models build-docker \\\n",
    "  -m ../experimentation/mlruns/2/2ce79d559d0546dd91bd26a252b0ac78/artifacts/model \\\n",
    "  -n edlongbottom/mlwebservice/bostonhousing:0.0.2 \\\n",
    "  --enable-mlserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the docker image (powershell)\n",
    "!mlflow models build-docker `\n",
    "  -m ../experimentation/mlruns/2/2ce79d559d0546dd91bd26a252b0ac78/artifacts/model `\n",
    "  -n edlongbottom/mlwebservice/bostonhousing:0.0.2 `\n",
    "  --enable-mlserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7db427",
   "metadata": {},
   "source": [
    "Serve the built image on Docker and map the web service port (8080) to localhost port (1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d719df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it -p 1234:8080 --name test-ml-model edlongbottom/mlwebservice/bostonhousing:0.0.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a01b44",
   "metadata": {},
   "source": [
    "Test the prediction service API using the same request format as with the local REST server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92803d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST -H \"Content-Type:application/json; format=pandas-split\" --data \"{\\\"columns\\\":[\\\"INDUS\\\",\\\"RM\\\",\\\"TAX\\\",\\\"PTRATIO\\\",\\\"LSTAT\\\"],\\\"data\\\":[[5.86, 6.108, 330.0, 19.1, 9.16]]}\" http://127.0.0.1:1234/invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f84d29",
   "metadata": {},
   "source": [
    "Tear down the service once testing is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "201cc457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-ml-model\n",
      "test-ml-model\n"
     ]
    }
   ],
   "source": [
    "!docker stop test-ml-model\n",
    "!docker rm test-ml-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb80d2",
   "metadata": {},
   "source": [
    "### Deploy the model to Kubernetes using Seldon Core\n",
    "\n",
    "Set the current context in your kubectl CLI to the chosen Kubernetes cluster. And if it doesn't already have it installed, install Seldon Core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dedicated namespace for seldon core\n",
    "!kubectl create namespace seldon-system\n",
    "\n",
    "# use helm to install seldon-core from the template helm chart\n",
    "!helm install seldon-core seldon-core-operator `\n",
    "    --repo https://storage.googleapis.com/seldon-charts `\n",
    "    --set usageMetrics.enabled=true `\n",
    "    --set ambassador.enabled=true `\n",
    "    --namespace seldon-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aace78",
   "metadata": {},
   "source": [
    "Next, install Ambassador API gateway on kubernetes to route requests to our model(s).\n",
    "\n",
    "Note: currently not working (Ambassador helm chart not compatible with version of k8s running on Docker Desktop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09481299",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (Temp/ipykernel_7920/2990590381.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\eddlo\\AppData\\Local\\Temp/ipykernel_7920/2990590381.py\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    --set image.repository=docker.io/datawire/ambassador `\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# add the repo for ambassador (datawire) to your helm repos config\n",
    "!helm repo add datawire https://www.getambassador.io\n",
    "!helm repo update\n",
    "    \n",
    "# install the ambassador helm chart\n",
    "!helm install ambassador datawire/ambassador `\n",
    "    --set image.repository=docker.io/datawire/ambassador `\n",
    "    --set crds.keep=false `\n",
    "    --set enableAES=false `\n",
    "    --namespace seldon-system\n",
    "\n",
    "# map localhost port 8003 to port 8080 on the API gateway on k8s\n",
    "!kubectl port-forward $(kubectl get pods -n seldon-system -l app.kubernetes.io/name=ambassador -o jsonpath='{.items[0].metadata.name}') -n seldon-system 8003:8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee84dbc",
   "metadata": {},
   "source": [
    "Next, deploy the containerised model to a Kubernetes cluster using Helm. Seldon Core have helm chart templates that can be used for the deployment, or alternatively the MLFlow website has an example YAML manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6566fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dedicated namespace for model serving\n",
    "!kubectl create namespace model-serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7a99f",
   "metadata": {},
   "source": [
    "Deploy the model using a YAML manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de06a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy the containerised model using spec defined in a deployment.yaml manifest\n",
    "!kubectl apply -f ./mlflow-housing/deployment/deployment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab81e9b8",
   "metadata": {},
   "source": [
    "OR, deploy using a Helm chart (ISSUE - failed calling webhook - unresolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install a helm chart to serve the model\n",
    "!helm install test-ml-seldon-app seldon-single-model `\n",
    "  --repo https://storage.googleapis.com/seldon-charts `\n",
    "  --set model.image=edlongbottom/mlwebservice/bostonhousing:0.0.2 `\n",
    "  --namespace model-serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7744b62",
   "metadata": {},
   "source": [
    "Test the prediction web service using Curl or the python requests module\n",
    "\n",
    "Note: currently NOT working, could be down to following issues:\n",
    " - Incorrectly formatted curl request\n",
    " - Problems with hosting a service at localhost port 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8136afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up seldon client\n",
    "sc = SeldonClient(\n",
    "    deployment_name=\"mlflow-model\",\n",
    "    namespace=\"model-serving\",\n",
    "    gateway_endpoint=\"localhost:8003\",\n",
    "    gateway=\"ambassador\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d7fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sc.predict(transport=\"rest\")\n",
    "assert r.success == True\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f578bf87",
   "metadata": {},
   "source": [
    "Use Curl to test requests to the prediction service end-point exposed at localhost port 8003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ddba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first retrieve the IP for the API gateway (will just be localhost for Docker Desktop)\n",
    "!kubectl -n ambassador get service ambassador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0c8a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now pass a request using curl (in Linux)\n",
    "# the URL follows template - http://<ambassadorEndpoint>/seldon/<namespace>/<deploymentName>/api/v0.1/predictions\n",
    "!curl http://localhost:8003/seldon/model-serving/mlflow-model/api/v0.1/predictions \\\n",
    "    --request POST \\\n",
    "    --header \"Content-Type: application/json\" \\\n",
    "    --data '{\"data\":{\"names\":[\"INDUS\",\"RM\",\"TAX\",\"PTRATIO\",\"LSTAT\"],\"tensor\":{\"shape\":[5,1],\"values\":[-0.77089554,-0.2106905 ,-0.46459208,0.27510008,-0.53194571]}}}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Curl to test the web service (in windows)\n",
    "!curl -X POST -H \"Content-Type:application/json\" --data \"{\\\"data\\\":{\\\"names\\\":[\\\"INDUS\\\",\\\"RM\\\",\\\"TAX\\\",\\\"PTRATIO\\\",\\\"LSTAT\\\"],\\\"tensor\\\":{\\\"shape\\\":[5,1],\\\"values\\\":[-0.77089554,-0.2106905 ,-0.46459208,0.27510008,-0.53194571]}}}\" http://localhost:80/seldon/model-serving/mlflow-model/api/v0.1/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3f4a47",
   "metadata": {},
   "source": [
    "### Tear down resources in Kubernetes\n",
    "\n",
    "Once you are finished testing/using the web service, remove resources all from the Kubernetes instance to tidy up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4e31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the model deployment resources (if used YAML manifest)\n",
    "!kubectl delete SeldonDeployment mlflow-model -n model-serving\n",
    "!kubectl delete namespace model-serving\n",
    "\n",
    "# delete the model deployment resources (if used Helm chart)\n",
    "!helm uninstall test-ml-seldon-app --namepace model-serving\n",
    "!kubectl delete namespace model-serving\n",
    "\n",
    "# uninstall the API gateway\n",
    "!helm uninstall ambassador --namespace ambassador\n",
    "!kubectl delete namespace ambassador\n",
    "\n",
    "# uninstall the seldon core operator\n",
    "!helm uninstall seldon-core --namespace seldon-system\n",
    "!kubectl delete namespace seldon-system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-housing",
   "language": "python",
   "name": "venv-housing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
